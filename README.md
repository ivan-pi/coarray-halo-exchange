## Parallel Halo Exchange Using Fortran Coarrays

This repository contains code and test cases for investigating Fortran
coarray implementations of a parallel halo exchange operation associated
with domain decomposition methods for PDE. Here the domain is partitioned
into *P* subdomains, one for each of *P* processes (or images), and each
process calculates the unknown variables defined on its subdomain.
Processes need to exchange values along their subdomain boundary with
their neighbors in a *halo exchange* operation.

>I'd very much welcome receiving results from different platforms and/or
compilers than those given below. There is test data for a quite a few
different numbers of images/process, and I can generate others if asked.
I also welcome comments on the code, and other coarray approaches that
might be more efficient. If you have any thoughts on why Intel performs
so poorly, I'd especially like to hear those.

### Background
The particular form of the halo exchange considered here originates from the
MPI-parallel code [Truchas](https://gitlab.com/truchas/truchas). Abstractly
one starts with a (global) index set {1, 2, ..., *N*}. Given *P* processes,
an index is assigned to a unique process according to a block partition of
the index set: the first *N1* indices are assigned to process 1, the next
*N2* to process 2, etc. An index so assigned to a process is said to be
*owned* by that process. In addition to these, a process may include indices
owned by other processes. The owned indices are said to be *on-process* and
these additional indices, if any, are said to be *off-process*. For the
purposes of computation, the collection of all indices known to a process
are mapped to a process-local index set: the block of on-process indices are
mapped, in order, to consecutive local indices starting at 1, and the
off-process indices are mapped to consecutive local indices immediately
following these.

Now the elements of an array indexed by this index set will be distributed
across the *P* processes according to the distribution of the indices.
Each global index is owned by exactly one process (where it is on-process)
but may exist on other processes as an off-process index. The mapping from
off-process indices to their corresponding on-process index is a many-to-one
mapping. A fundamental operation on the distributed array is to replace the
value at each off-process index with the value at its corresponding
on-process index; this is referred to as a *gather* (of off-process data)
operation.

This distributed index set mapping and the associated gather operation
are implemented by the module [`index_map_type.f90`](index_map_type.f90).

### The Tests
The file [`main.f90`](main.f90) is a test driver. It reads data that
describes the partitioning of the index set and then performs the gather
operation on an integer array. The on-process elements of the array are
initialized with their corresponding global IDs and the off-process elements
with invalid data (-1). After the gather operation the off-process elements
should be filled with their global IDs, and this is checked. To get more
accurate timings the gather operation may be repeated multiple times.

The test data is stored in subdirectories of the `test-data` directory,
one for each test. A subdirectory contains a collection of input files,
one per image. Each file (unformatted stream) consists of 2 records. The
first consists of the block size assigned to the image (i.e., the number
of on-process indices) and the number of off-process indices. The second
record is the global IDs of the off-process indices in strictly increasing
order.

The current test data was generated by a version of Truchas hacked to output
this internal data. It comes from an unstructured finite element type mesh
partitioned using METIS and corresponds to the cell index set (there are
also the node, face, and edge index sets that could be obtained). There is
data from a series of meshes ("opencalc-B") of increasing size:

  | Mesh | B0  | B1   | B2   | B3   | B4
  | ---- | --  | --   | --   | --   | --
  | Cells| 70K | 206K | 562K | 1.6M | 4.4M

Each mesh is partitioned into various numbers of partitions. The mesh and
number of partitions is reflected in the name of the test subdirectory.

The test executable takes 1 or 2 command line arguments. The first is the
path to the directory containing the data files for the test. The second
is the number of times to repeat the gather operation before collecting
timing data. If not specified it defaults to just 1. Only the gather
operation itself is timed.

Here is a summary of how to compile and run the tests for different
compilers:

* **Intel:**
  - `ifort -coarray -O3 index_map_type.f90 main.f90`
  - `export FOR_COARRAY_NUM_IMAGES=12`
  - `./a.out <arguments>`
* **GFortran/OpenCoarrays:**
  - `caf -O3 index_map_type.f90 main.f90`
  - `cafrun -n 12 ./a.out <arguments>`
* **NAG:**
  - `nagfor -coarray -O3 -f2018 index_map_type.F90 main.f90`
  - `export NAGFORTRAN_NUM_IMAGES=12`
  - `./a.out <arguments>`

### Results (updated 22 Jan)
These results were collected on a standalone Linux workstation with 32GB
of memory and a 12-core AMD Threadripper 2920X CPU.

* **ifort:** Intel Fortran and MPI 2021.4.0
* **gfortran:** GCC 11.2.0, MPICH 3.3.2, OpenCoarrays 2.9.2-13-g235167d
* **nagfor:** NAG Fortran 7.1.7103
* **MPI:** Baseline MPI implementation, GCC/gfortran 11.2.0, MPICH 3.3.2
  (see below)

test-#image | ifort | gfortran | nagfor | MPI
:---------: | :---: | :------: | :----: | :-:
B0-12 | 0.023 s | 0.033 s |  23e-6 s | 12e-6 s
B1-12 | 0.076 s | 0.061 s |  28e-6 s | 17e-6 s
B2-12 | 0.38 s  | 0.12 s  |  37e-6 s | 27e-6 s
B3-12 | 2.8 s   | 0.29 s  |  56e-6 s | 42e-6 s
B4-12 | 13 s    | 0.45 s  | 140e-6 s | 99e-6 s

#### Comments
* By default the Intel version was needlessly hammering the network, which
  resulted in *even larger* times than shown here. This was solved by setting
  the environment variables `I_MPI_FABRICS` and `I_MPI_DEVICE` to `shm`; see
  this [Intel page](https://www.intel.com/content/www/us/en/develop/documentation/fortran-compiler-oneapi-dev-guide-and-reference/top/optimization-and-programming-guide/coarrays-1/using-coarrays.html).
* Intel misidentifies the layout of this processor as 6-cores with
  4 hardware threads per core. I used

      export I_MPI_PIN_PROCESSOR_LIST=0-11

  which appears to properly assign processes to cores (up to 12 processes).
  However it made little actual difference in the ultimate timings.


### Reference MPI Implementation

An MPI implementation of the halo exchange is found in the `mpi` directory.
This serves as a baseline against which to assess the different coarray
implementations. It uses a graph communicator and MPI-3 neighborhood
collective to perform the halo exchange.

Compiling and running the MPI version of the test goes something like
```
mpif90 -O3 mpi/index_map_type.f90 mpi/main.f90
mpirun -np 12 ./a.out <arguments>
```
#### Baseline Timings

These results were collected on a standalone Linux workstation with 32GB
of memory and a 12-core AMD Threadripper 2920X CPU.

* **mpich:**   GCC/gfortran 11.2.0 and MPICH 3.3.2
* **openmpi:** GCC/gfortran 11.2.0 and OpenMPI 4.1.2
* **intel:**   Intel Fortran/MPI 2021.4.0

test-#image | mpich | openmpi | intel
:---------: | :---: | :-----: | :---:
B0-12 | 12 µs | 13 µs |  71 µs
B1-12 | 17 µs | 26 µs |  69 µs
B2-12 | 27 µs | 45 µs |  77 µs
B3-12 | 42 µs | 68 µs | 100 µs
B4-12 | 99 µs | 99 µs | 150 µs

##### Comments
* Don't take the times too literally. It's hard to time something that
  takes such little time. They were collected using the `system_clock`
  intrinsic and averaged 10,000 iterations of the gather procedure.
* The Intel-built test was needlessly hammering on the network which likely
  accounts for at least some of its significantly larger times. The settings
  that kept the coarray version from doing the same thing, don't work/don't
  apply here.
